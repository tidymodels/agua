% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/h2o_train.R
\name{h2o_train}
\alias{h2o_train}
\alias{h2o_train_rf}
\alias{h2o_train_xgboost}
\alias{h2o_train_gbm}
\alias{h2o_train_glm}
\alias{h2o_train_nb}
\alias{h2o_train_mlp}
\alias{h2o_train_rule}
\alias{h2o_train_auto}
\title{Model wrappers for h2o}
\usage{
h2o_train(x, y, model, weights = NULL, validation = NULL, ...)

h2o_train_rf(x, y, ntrees = 50, mtries = -1, min_rows = 1, ...)

h2o_train_xgboost(
  x,
  y,
  ntrees = 50,
  max_depth = 6,
  min_rows = 1,
  learn_rate = 0.3,
  sample_rate = 1,
  col_sample_rate = 1,
  min_split_improvement = 0,
  stopping_rounds = 0,
  validation = NULL,
  ...
)

h2o_train_gbm(
  x,
  y,
  ntrees = 50,
  max_depth = 6,
  min_rows = 1,
  learn_rate = 0.3,
  sample_rate = 1,
  col_sample_rate = 1,
  min_split_improvement = 0,
  stopping_rounds = 0,
  ...
)

h2o_train_glm(x, y, lambda = NULL, alpha = NULL, ...)

h2o_train_nb(x, y, laplace = 0, ...)

h2o_train_mlp(
  x,
  y,
  hidden = 200,
  l2 = 0,
  hidden_dropout_ratios = 0,
  epochs = 10,
  activation = "Rectifier",
  validation = NULL,
  ...
)

h2o_train_rule(
  x,
  y,
  rule_generation_ntrees = 50,
  max_rule_length = 5,
  lambda = NULL,
  ...
)

h2o_train_auto(x, y, verbosity = NULL, ...)
}
\arguments{
\item{x}{A data frame of predictors.}

\item{y}{A vector of outcomes.}

\item{model}{A character string for the model. Current selections are
<<<<<<< HEAD
\code{"automl"}, \code{"randomForest"}, \code{"xgboost"}, \code{"glm"}, \code{"deeplearning"}, \code{"rulefit"} and
=======
\code{"randomForest"}, \code{"xgboost"}, "\code{gbm}", \code{"glm"}, \code{"deeplearning"}, \code{"rulefit"} and
>>>>>>> gbm
\code{"naiveBayes"}. Use \code{\link[h2o:h2o.xgboost.available]{h2o::h2o.xgboost.available()}} to see if xgboost
can be used on your OS/h2o server.}

\item{weights}{A numeric vector of case weights.}

\item{validation}{The \emph{proportion} of the data that are used for performance
assessment and potential early stopping.}

\item{...}{Other options to pass to the h2o model functions (e.g.,
\code{\link[h2o:h2o.randomForest]{h2o::h2o.randomForest()}}).}

\item{ntrees}{Number of trees. Defaults to 50.}

\item{mtries}{Number of variables randomly sampled as candidates at each split. If set to -1, defaults to sqrt{p} for
classification and p/3 for regression (where p is the # of predictors Defaults to -1.}

\item{min_rows}{Fewest allowed (weighted) observations in a leaf. Defaults to 1.}

\item{max_depth}{Maximum tree depth (0 for unlimited). Defaults to 20.}

\item{learn_rate}{(same as eta) Learning rate (from 0.0 to 1.0) Defaults to 0.3.}

\item{sample_rate}{Row sample rate per tree (from 0.0 to 1.0) Defaults to 0.632.}

\item{col_sample_rate}{(same as colsample_bylevel) Column sample rate (from 0.0 to 1.0) Defaults to 1.}

\item{min_split_improvement}{Minimum relative improvement in squared error reduction for a split to happen Defaults to 1e-05.}

\item{stopping_rounds}{Early stopping based on convergence of stopping_metric. Stop if simple moving average of length k of the
stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable) Defaults to 0.}

\item{lambda}{Regularization strength}

\item{alpha}{Distribution of regularization between the L1 (Lasso) and L2 (Ridge) penalties. A value of 1 for alpha
represents Lasso regression, a value of 0 produces Ridge regression, and anything in between specifies the
amount of mixing between the two. Default value of alpha is 0 when SOLVER = 'L-BFGS'; 0.5 otherwise.}

\item{laplace}{Laplace smoothing parameter Defaults to 0.}

\item{hidden}{Hidden layer sizes (e.g. [100, 100]). Defaults to c(200, 200).}

\item{l2}{L2 regularization (can add stability and improve generalization, causes many weights to be small. Defaults to
0.}

\item{hidden_dropout_ratios}{Hidden layer dropout ratios (can improve generalization), specify one value per hidden layer, defaults to 0.5.}

\item{epochs}{How many times the dataset should be iterated (streamed), can be fractional. Defaults to 10.}

\item{activation}{Activation function. Must be one of: "Tanh", "TanhWithDropout", "Rectifier", "RectifierWithDropout", "Maxout",
"MaxoutWithDropout". Defaults to Rectifier.}

\item{rule_generation_ntrees}{Specifies the number of trees to build in the tree model. Defaults to 50. Defaults to 50.}

\item{max_rule_length}{Maximum length of rules. Defaults to 3.}

\item{verbosity}{Verbosity of the backend messages printed during training; Optional.
Must be one of NULL (live log disabled), "debug", "info", "warn", "error". Defaults to "warn".}
}
\value{
An h2o model object.
}
\description{
Basic model wrappers for h2o model functions that include data conversion,
seed configuration, and so on.
}
\examples{
# start with h2o::h2o.init()

if (h2o_running()) {
  # -------------------------------------------------------------------------
  # Using the model wrappers:
  h2o_train_glm(mtcars[, -1], mtcars$mpg)

  # -------------------------------------------------------------------------
  # using parsnip:

  spec <-
    rand_forest(mtry = 3, trees = 1000) \%>\%
    set_engine("h2o") \%>\%
    set_mode("regression")

  set.seed(1)
  mod <- fit(spec, mpg ~ ., data = mtcars)
  mod

  predict(mod, head(mtcars))
}
}
